{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1855160,"sourceType":"datasetVersion","datasetId":1103582},{"sourceId":1855180,"sourceType":"datasetVersion","datasetId":1103594},{"sourceId":1862818,"sourceType":"datasetVersion","datasetId":1104323}],"dockerImageVersionId":30043,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Description\nI implemented an autoencoder for 3D point cloud (number of points x 3) data,\n* using TensorFlow\n* applying (modified) Self-Attention Layer of Transformer  \n\nThough this notebook was created for my own study, I wish this notebook would help someone.  \n\nNote: This code requires GPU usage. Pretrained weights are input by default.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"## Refferences\n[1] https://arxiv.org/abs/2012.09688  \n  Details of the architecture are described and the core code is uploaded to github. It was so helpful for me. \n  \n[2] https://arxiv.org/abs/2012.09164  \n  SOTA model for 3D Point Cloud classification at Jan 2021. \"Vector attention\" is adopted.  \n  \n[3] https://arxiv.org/abs/1712.07262  \n  Point cloud Auto-encoder","metadata":{}},{"cell_type":"markdown","source":"# Results Visualization  \nBelow is an animation in which one shape of airplane changes into another shape and repeats that. Intermediate images of the animation are made by interpolation, and two interpolation methods are compared:\n* left: interpolate the coordinates of point clouds directly\n* right: interpolate encoded features and decode to point cloud","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nnbdatapath = \"../input/transformerbased-autoencoder-for-3d-point-cloud\"\nImage(filename=nbdatapath+\"/Transformer_AE_v003.gif\",\n      format='png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the feature extracted in the learned autoencoder has important information about the shape, interpolating it provides natural shapeshift animation (right) even fed random order data.","metadata":{}},{"cell_type":"markdown","source":"## Preparation\n* https://github.com/AnTao97/PointCloudDatasets  \n\nI Used \"ShapeNetPart\" dataset. It consists of 16 categories: ","metadata":{}},{"cell_type":"code","source":"Image(nbdatapath+\"/shapenetpart_shapes.jpg\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_list = ['airplane','bag','cap','car','chair',\n            'earphone','guitar','knife','lamp',\n            'laptop','motorbike','mug','pistol',\n            'rocket','skateboard','table']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Farthest point sampling\" algorithm is used in this code. Processing on the GPU needs to run the shell script.","metadata":{}},{"cell_type":"code","source":"### \"tf_ops\" folder was downloaded from https://github.com/dgriffiths3/pointnet2-tensorflow2\n### The following command must be run with the GPU on.\n!sh ../input/pointnet2-tf-ops/tf_ops/compile_ops.sh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# random.seed(0)\nfeat_dims = 1024\nbatch_size = 32\nver_label = 'Transformer_AE_v003'\ncheckpoint_dir = 'CPs_'+ver_label\n\ntrain_epochs = 0 # if ==0, skip training\nload_weight = True # if True, start from epoch 200\n\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport gc, sys, glob\nfrom tqdm import tqdm\n\nfrom tensorflow.keras import Input\nfrom tensorflow.keras import models as M\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras import backend as keras\nfrom tensorflow.keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\nfrom matplotlib.animation import FuncAnimation\nimport seaborn as sns\nsns.set(style='darkgrid')\n\nimport h5py\nfrom sklearn.manifold import TSNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampling_module=tf.load_op_library('./tf_sampling_so.so')\nfrom tensorflow.python.framework import ops\nops.NoGradient('FarthestPointSample')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_h5(h5_filename):\n    f = h5py.File(h5_filename, 'r')\n    data = f['data'][:]\n    label = f['label'][:]\n    return (data, label)\n\ndef read_file(x_str):\n    \n    h5_fns = sorted(glob.glob('../input/shapenetpart-hdf5-2048/'\n                              +x_str+'*.h5'))\n    list_temp1, list_temp2 = [],[]\n    for h5_filename in tqdm(h5_fns):\n        h5 = load_h5(h5_filename)\n        list_temp1.append(h5[0])\n        list_temp2.append(h5[1])\n    X = np.concatenate(list_temp1, axis=0)\n    X_label = np.concatenate(list_temp2, axis=0).squeeze()\n    \n    return X, X_label\n\nX, X_label = read_file('train')\nXv, Xv_label = read_file('val')\nXt, Xt_label = read_file('test')\n    \nprint(f'train:{X.shape}, val:{Xv.shape}, test:{Xt.shape}')\nnum_points = X.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shuffle the order of data in each cloud in advance. This clarifies that the model is not affected by data order.","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)\nfor items in [X, Xv, Xt]:\n    for values in tqdm(items):\n        np.random.shuffle(values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_label = [cat_list[X_label[i]]\n             for i in range(X_label.shape[0])]\ncat_label_v = [cat_list[Xv_label[i]]\n               for i in range(Xv_label.shape[0])]\ncat_label_t = [cat_list[Xt_label[i]]\n               for i in range(Xt_label.shape[0])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def PCPlot3d(ax, pts):\n\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_zlabel(\"Z\")\n\n    X,Y,Z = pts[:,0], -pts[:,2], pts[:,1]\n    ax.set_xlim(-1., 1.)\n    ax.set_ylim(-1., 1.)\n    ax.set_zlim(-1., 1.)\n\n    ax.plot(X,Y,Z, marker=\"o\", markersize=1, linestyle='None')\n\n%matplotlib inline\nfig = plt.figure(figsize=(5,5))\nfig.tight_layout()\nax = Axes3D(fig)\nPCPlot3d(ax, X[333])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder Construction\nI tried to follow the architecture of [1] almost closely, but I avoided using BatchNorm layers due to performance degradation.  \nPlease note the encoder is not completely consistent in structure with the original Transoformer.","metadata":{}},{"cell_type":"markdown","source":"Since point cloud data contains no information in  order,\n* It is suitable to apply attention layer which is permutation-invariant.\n* Positional embedding which is applied in the original Transformer can be discarded.","metadata":{}},{"cell_type":"code","source":"def pairwise_distance(xyz1, xyz2):\n    n = xyz1.shape[1]\n    c = xyz1.shape[2]\n    m = xyz2.shape[1]\n    xyz1 = tf.tile(tf.reshape(xyz1, (-1,1,n,c)), [1,m,1,1])\n    xyz2 = tf.tile(tf.reshape(xyz2, (-1,m,1,c)), [1,1,n,1])\n    dist = tf.reduce_sum((xyz1-xyz2)**2, -1)\n    return dist\n\ndef knn_point(k, xyz1, xyz2):\n    dist = -pairwise_distance(xyz1, xyz2)\n    val, idx = tf.math.top_k(dist, k)\n    return -val, idx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def LayerLinBnRelu(tensor, C, seq_name,\n                   use_bias=True, activation=None, LeakyAlpha=0.0):\n    x_in = Input(shape=tensor.shape[1:], name=seq_name+'_input')\n    x = L.Dense(C, use_bias=use_bias, activation=activation,\n                name=seq_name+'_lin')(x_in)\n#     x = L.BatchNormalization(name=seq_name+'_bn')(x)\n    if LeakyAlpha==0.0:\n        x_out = L.ReLU(name=seq_name+'_ReLU')(x)\n    else:  \n        x_out = L.LeakyReLU(alpha=LeakyAlpha,\n                            name=seq_name+'_ReLU')(x)\n    model = M.Model(inputs=x_in, outputs=x_out, name=seq_name)\n    return model(tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_and_group(args, nsample):\n    xyz, pts, fps_idx = args\n\n    new_xyz = tf.gather_nd(xyz, tf.expand_dims(fps_idx,-1), batch_dims=1)\n    new_pts = tf.gather_nd(pts, tf.expand_dims(fps_idx,-1), batch_dims=1)\n    _, idx = knn_point(nsample, xyz, new_xyz)\n\n#     grouped_xyz = tf.gather_nd(xyz, tf.expand_dims(idx,-1), batch_dims=1)\n#     grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2),\n#                            (1,1,nsample,1))\n    grouped_pts = tf.gather_nd(pts, tf.expand_dims(idx,-1), batch_dims=1)\n    grouped_pts -= tf.tile(tf.expand_dims(new_pts, 2),\n                           (1,1,nsample,1))\n\n    new_pts = tf.concat([grouped_pts,\n                         tf.tile(tf.expand_dims(new_pts, 2),\n                                 (1,1,nsample,1))],\n                        axis=-1)\n    \n    return new_xyz, new_pts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def LayerSelfAttention(tensor, seq_name):\n    x_in = Input(shape=tensor.shape[1:], name=seq_name+'_input')\n\n    C = x_in.shape[2]\n    W_q = L.Dense(C//4, use_bias=False, activation=None,\n                  name=seq_name+'_Q')\n    W_k = L.Dense(C//4, use_bias=False, activation=None,\n                  name=seq_name+'_K')\n    # W_v has bias in the original code,\n    # but set no bias here as well as W_q and W_k.\n    W_v = L.Dense(C, use_bias=False, activation=None,\n                  name=seq_name+'_V')\n\n    x_q = W_q(x_in)\n    x_k = W_k(x_in)\n    W_k.set_weights(W_q.get_weights())\n    x_k = L.Lambda(lambda t: tf.transpose(t, perm=(0,2,1)),\n                   name=seq_name+'_KT')(x_k)\n    x_v = W_v(x_in)\n\n    energy = L.Lambda(lambda ts: tf.matmul(ts[0],ts[1]),\n                      name=seq_name+'_matmul1')([x_q, x_k])\n    attention = L.Softmax(axis=1, name=seq_name+'_softmax')(energy)\n    attention = L.Lambda(lambda t:\n                         t / (1e-9 + tf.reduce_sum(t, axis=2,\n                                                   keepdims=True)),\n                         name=seq_name+'_l1norm')(attention)\n\n    x_r = L.Lambda(lambda ts: tf.matmul(ts[0],ts[1]),\n                   name=seq_name+'_matmul2')([attention, x_v])\n    x_r = L.Lambda(lambda ts: tf.subtract(ts[0],ts[1]),\n                   name=seq_name+'_subtract')([x_in, x_r])\n    x_r = LayerLinBnRelu(x_r, C, seq_name+'_LBR', use_bias=True)\n    x_out = L.Lambda(lambda ts: tf.add(ts[0],ts[1]),\n                     name=seq_name+'_add')([x_in, x_r])\n\n    model = M.Model(inputs=x_in, outputs=x_out, name=seq_name)\n    return model(tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## ENCODER ##\n\nxyz = Input(shape=(num_points, 3), name='input_points')\nx = LayerLinBnRelu(xyz, 64, 'E-IN_LBR1', use_bias=False)\nx = LayerLinBnRelu(x, 64, 'E-IN_LBR2', use_bias=False)\n\n# sample and group(SG) Module #1\nfps_idx = L.Lambda(sampling_module.farthest_point_sample,\n                   arguments={'npoint':512},\n                   name='E-FPS_IDX1')(xyz)\nnew_xyz, new_feature = L.Lambda(sample_and_group,\n                                arguments={'nsample':32},\n                                name='E-SG1')([xyz, x, fps_idx])\nx = LayerLinBnRelu(new_feature, 128, 'E-SG1_LBR1', use_bias=False)\nx = LayerLinBnRelu(x, 128, 'E-SG1_LBR2', use_bias=False)\nx = L.Lambda(lambda t: tf.reduce_max(t, axis=2),\n             name='E-SG1_MaxPool')(x)\n\n# sample and group(SG) Module #2\nfps_idx = L.Lambda(sampling_module.farthest_point_sample,\n                   arguments={'npoint':256},\n                   name='E-FPS_IDX2')(new_xyz)\nnew_xyz, new_feature = L.Lambda(sample_and_group,\n                                arguments={'nsample':32},\n                                name='E-SG2')([new_xyz, x, fps_idx])\nx = LayerLinBnRelu(new_feature, 256, 'E-SG2_LBR1', use_bias=False)\nx = LayerLinBnRelu(x, 256, 'E-SG2_LBR2', use_bias=False)\nx = L.Lambda(lambda t: tf.reduce_max(t, axis=2),\n             name='E-SG2_MaxPool')(x)\n\n# Self Attention\nx1 = LayerSelfAttention(x, 'E-SA1')\nx2 = LayerSelfAttention(x1, 'E-SA2')\nx3 = LayerSelfAttention(x2, 'E-SA3')\nx4 = LayerSelfAttention(x3, 'E-SA4')\nx0 = L.Lambda(lambda ts: tf.concat(ts, axis=2),\n              name='E-SA_Concat')([x1,x2,x3,x4])\n\n# In the original code, input embedding is also concatenated.\n# (but this wasn't illustrated in Figure 2 in the paper.)\nx = L.Lambda(lambda ts: tf.concat(ts, axis=2),\n             name='E-OUT_Concat')([x0,x])\n\nx = LayerLinBnRelu(x, feat_dims, 'E-OUT_LBR',\n                   use_bias=False, LeakyAlpha=0.2)\noutput_feats = L.Lambda(lambda t: tf.reduce_max(t, axis=1, keepdims=True),\n                        name='E-OUT_MaxPool')(x)\n\nPCT_Encoder = M.Model(inputs=xyz, outputs=output_feats)\nPCT_Enc_list = [layer.name for layer in PCT_Encoder.layers]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder Visualization\nplot_model(1): Visualize the architecture of whole encoder.","metadata":{}},{"cell_type":"code","source":"plot_model(PCT_Encoder, show_shapes=True, dpi=96,\n           to_file='model_enc1_whole.png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot_model(2): Visualize inside of the \"LinBnRelu(LBR)\" layer. (Actually \"LinRelu\" is correct...)","metadata":{}},{"cell_type":"code","source":"# plotting inside of the layer: \"LinBnRelu(LBR)\" layer\ni = PCT_Enc_list.index('E-IN_LBR1')\nplot_model(PCT_Encoder.layers[i], show_shapes=True, dpi=96,\n           to_file='model_enc2_LBR.png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot_model(3): Visualize inside of the Self Attention layer.","metadata":{}},{"cell_type":"code","source":"# plotting inside of the layer: Self Attention layer\ni = PCT_Enc_list.index('E-SA1')\nplot_model(PCT_Encoder.layers[i], show_shapes=True, dpi=96,\n           to_file='model_enc3_SelfAttention.png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decoder Construction\nConventional decoder part of autoencoder seems to be constructed of cascaded fully connected layers. But I tried to adopt source-target attention layer, following the original Transformer.  \n\nIn the paper [3] the fixed grid points are input to decoder part, so I thought feeding some fixed value into the decoder can be available for transformer autoencoder as well. I chose stacked identity matrix (tf.eye) almost intuitively.","metadata":{}},{"cell_type":"code","source":"def LayerSrcTrgtAttention(args, seq_name):\n    E_tensor, D_tensor = args\n    xE_in = Input(shape=E_tensor.shape[1:], name=seq_name+'_input-E')\n    C = xE_in.shape[2]\n    \n    xD_in = Input(shape=D_tensor.shape[1:], name=seq_name+'_input-D')\n    out_dim = xD_in.shape[2]\n\n    W_q = L.Dense(C//4, use_bias=False, activation=None,\n                  name=seq_name+'_Q')\n    W_k = L.Dense(C//4, use_bias=False, activation=None,\n                  name=seq_name+'_K')\n    # W_v has bias in the original code,\n    # but set no bias here as well as W_q and W_k.\n    W_v = L.Dense(out_dim, use_bias=False, activation=None,\n                  name=seq_name+'_V')\n\n    x_q = W_q(xD_in)\n    x_k = W_k(xE_in)\n#     W_k.set_weights(W_q.get_weights())\n    x_k = L.Lambda(lambda t: tf.transpose(t, perm=(0,2,1)),\n                   name=seq_name+'_KT')(x_k)\n    x_v = W_v(xE_in)\n\n    energy = L.Lambda(lambda ts: tf.matmul(ts[0],ts[1]),\n                      name=seq_name+'_matmul1')([x_q, x_k])\n    attention = L.Softmax(axis=1, name=seq_name+'_softmax')(energy)\n    attention = L.Lambda(lambda t:\n                         t / (1e-9 + tf.reduce_sum(t, axis=2,\n                                                   keepdims=True)),\n                         name=seq_name+'_l1norm')(attention)\n\n    x_r = L.Lambda(lambda ts: tf.matmul(ts[0],ts[1]),\n                   name=seq_name+'_matmul2')([attention, x_v])\n    x_r = L.Lambda(lambda ts: tf.subtract(ts[0],ts[1]),\n                   name=seq_name+'_subtract')([xD_in, x_r])\n    x_r = LayerLinBnRelu(x_r, out_dim, seq_name+'_LBR',\n                         use_bias=True)\n    x_out = L.Lambda(lambda ts: tf.add(ts[0],ts[1]),\n                     name=seq_name+'_add')([xD_in, x_r])\n\n    model = M.Model(inputs=[xE_in,xD_in],\n                    outputs=x_out, name=seq_name)\n    return model([E_tensor,D_tensor])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def copy_and_mapping(tensor, nmul, seq_name):\n    x_in = Input(shape=tensor.shape[1:], name=seq_name+'_input')\n    x = L.Lambda(lambda t: tf.expand_dims(t, 2),\n                 name=seq_name+'_expand')(x_in)\n    C = x.shape[-1]//nmul\n    x1 = L.Conv2DTranspose(C,(1,nmul),(1,nmul),\n                           use_bias=True, activation=None,\n                           name=seq_name+'_convT')(x)\n    x2 = L.Dense(C, use_bias=True, activation=None,\n                 name=seq_name+'_lin')(x)\n    x2 = L.Lambda(lambda t: tf.tile(t, [1,1,nmul,1]),\n                  name=seq_name+'_tile')(x2)\n    x = L.Lambda(lambda ts: tf.add(ts[0],ts[1]),\n                 name=seq_name+'_add')([x1, x2])\n    npoint = x.shape[1]*x.shape[2]\n    x_out = L.Lambda(lambda t: tf.reshape(t, [-1,npoint,t.shape[3]]),\n                     name=seq_name+'_reshape')(x)\n    model = M.Model(inputs=x_in, outputs=x_out, name=seq_name)\n    return model(tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### DECODER ###\ninput_feats = Input(shape=(1, feat_dims), name='input_feats')\nm_feats = L.Lambda(lambda x: tf.tile(x, [1,256,1]),\n                   name = 'D-IN_replicate')(input_feats)\n\ninput_eye_seed = Input(shape=(1,1), name='input_eye_seed')\n# Make batch_size*eye tensor using broadcast\ninput_eye = input_eye_seed + tf.eye(256,256)\nx = L.Dense(feat_dims//4, use_bias=False, activation=None,\n            name='D-IN')(input_eye)\n\n# Source Target Attention\nx1 = LayerSrcTrgtAttention([m_feats,x], 'D-STA1')\nx2 = LayerSrcTrgtAttention([m_feats,x1], 'D-STA2')\nx3 = LayerSrcTrgtAttention([m_feats,x2], 'D-STA3')\nx4 = LayerSrcTrgtAttention([m_feats,x3], 'D-STA4')\n\nx0 = L.Lambda(lambda ts: tf.concat(ts, axis=2),\n              name='D-STA_Concat')([x1,x2,x3,x4])\nx = L.Lambda(lambda ts: tf.concat(ts, axis=2),\n             name='D-OUT_Concat')([x0,x])\n\nx = copy_and_mapping(x, 8, 'D-OUT_CopyAndMapping')\nx = LayerLinBnRelu(x, 64, 'D-OUT_LBR1', use_bias=False)\nx = LayerLinBnRelu(x, 64, 'D-OUT_LBR2', use_bias=False,\n                   LeakyAlpha=0.2)\n\noutput_points = L.Dense(3, activation=None, name='D-OUT_lin')(x)\n\nPCT_Decoder = M.Model(inputs=[input_feats,input_eye_seed],\n                      outputs=output_points)\nPCT_Dec_list = [layer.name for layer in PCT_Decoder.layers]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decoder Visualization\nplot_model(4):  Visualize the architecture of whole decoder.","metadata":{}},{"cell_type":"code","source":"plot_model(PCT_Decoder, show_shapes=True, dpi=96,\n           to_file='model_dec1_whole.png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot_model(5): Visualize inside of the Source-Target Attention layer.","metadata":{}},{"cell_type":"code","source":"# plotting inside of the layer: Source Target Attention layer\ni = PCT_Dec_list.index('D-STA1')\nplot_model(PCT_Decoder.layers[i], show_shapes=True, dpi=96,\n           to_file='model_dec2_SourceTargetAttention.png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model summary","metadata":{}},{"cell_type":"code","source":"PCT_Encoder.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PCT_Decoder.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Prediction","metadata":{}},{"cell_type":"code","source":"AE = M.Model(inputs=[PCT_Encoder.input, input_eye_seed],\n             outputs=PCT_Decoder([PCT_Encoder.output, input_eye_seed]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In many cases of evaluating similarity between original and reconstructed point cloud, chamfer distance is adopted as loss function, which is not affected by data order.","metadata":{}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/47060685/chamfer-distance-between-two-point-clouds-in-tensorflow/54767428\n# modified as follows:\n# 1) respond to cases that the size of array1 and array2 are different\n# 2) dtype=tf.float64 --> dtype=tf.float32\ndef distance_matrix(array1, array2):\n    _, num_features = array1.shape\n    expanded_array1 = tf.tile(array1, (array2.shape[0], 1))\n    expanded_array2 = tf.reshape(\n            tf.tile(tf.expand_dims(array2, 1), \n                    (1, array1.shape[0], 1)),\n            (-1, num_features))\n    distances = tf.norm(expanded_array1-expanded_array2, axis=1)\n    distances = tf.reshape(distances, (array2.shape[0], array1.shape[0]))\n    return distances\n\ndef av_dist(array1, array2):\n    distances = distance_matrix(array1, array2)\n    distances1 = tf.reduce_min(distances, axis=0)\n    distances1 = tf.reduce_mean(distances1)\n    distances2 = tf.reduce_min(distances, axis=1)\n    distances2 = tf.reduce_mean(distances2)\n    return distances1, distances2\n\ndef av_dist_sum(arrays):\n    array1, array2 = arrays\n    av_dist1, av_dist2 = av_dist(array1, array2)\n    return av_dist1+av_dist2\n\ndef chamfer_distance_tf(array1, array2):\n    # batch_size, num_point, num_features = array1.shape\n\n    dist = tf.reduce_mean(\n               tf.map_fn(av_dist_sum, elems=(array1, array2), dtype=tf.float32)\n           )\n    return dist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-3,\n    decay_steps=2000000//batch_size,\n    decay_rate=0.1)\nopt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\nAE.compile(optimizer=opt,\n           loss=chamfer_distance_tf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if load_weight:\n    load_dir = nbdatapath+'/pretrained'\n    latest = tf.train.latest_checkpoint(load_dir)\n    AE.load_weights(latest)\n    initial_epoch = int(os.path.basename(latest).lstrip('cp-').rstrip('.ckpt'))\nelse:\n    initial_epoch = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = checkpoint_dir + '/cp-{epoch:04d}.ckpt'\ndef eye_seed(X):\n    return tf.zeros([X.shape[0],1,1])\n\nif train_epochs>0:\n    cp = tf.keras.callbacks.ModelCheckpoint\n    cp_callback = cp(checkpoint_path, save_weights_only=True,\n                     verbose=1, period=10)\n    AE.fit([X, eye_seed(X)], X, \n           validation_data = ([Xv, eye_seed(Xv)], Xv),\n           batch_size=batch_size,\n           initial_epoch=initial_epoch,\n           epochs=train_epochs,\n           verbose=1, callbacks=[cp_callback])\n    latest = tf.train.latest_checkpoint(checkpoint_dir)\n\nprint('evaluation <train data>:')\nAE.evaluate([X, eye_seed(X)], X,\n            batch_size=batch_size, verbose=1)\nprint('evaluation <val data>:')\nAE.evaluate([Xv, eye_seed(Xv)], Xv,\n            batch_size=batch_size, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def AE_predict(X):\n    X_feat = PCT_Encoder.predict(X, batch_size=batch_size,\n                                 verbose=1)\n    X_pred = PCT_Decoder.predict([X_feat, eye_seed(X)],\n                                 batch_size=batch_size,\n                                 verbose=1)\n    X_feat = X_feat.squeeze()\n    return X_feat, X_pred\n\nprint('prediction <train data>:')\nX_feat, X_pred = AE_predict(X)\nprint('prediction <val data>:')\nXv_feat, Xv_pred = AE_predict(Xv)\nprint('prediction <test data>:')\nXt_feat, Xt_pred = AE_predict(Xt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Postprocess","metadata":{}},{"cell_type":"code","source":"def compare_plot(Xi, Xi_pred):\n    fig = plt.figure(figsize=(12,5))\n    fig.tight_layout()\n    ax = fig.add_subplot(121, projection='3d')\n    PCPlot3d(ax, Xi)\n    ax = fig.add_subplot(122, projection='3d')\n    PCPlot3d(ax, Xi_pred)\n    plt.subplots_adjust(left=0.05, right=0.95, bottom=0, top=1)\n    plt.show()\n\n%matplotlib inline\ni=15\ncompare_plot(X[i], X_pred[i])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Visualization with t-SNE\nBy memory limitation in kaggle environment, t-SNE requires data sampling.","metadata":{}},{"cell_type":"code","source":"%%time\nXall_feat = np.concatenate([X_feat[:4000],\n                            Xv_feat[:600],\n                            Xt_feat[:900]])\nXall_embd = TSNE(n_components=2, random_state=0,\n                 verbose=1).fit_transform(Xall_feat)\nX_embd, Xv_embd, Xt_embd = np.split(Xall_embd,\n                                    [4000,4000+600])\ndel Xall_feat, Xall_embd; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nfig = plt.figure(figsize=(6,18))\nfig.tight_layout()\n\naxs = [fig.add_subplot(311+i) for i in range(3)]\n\nfor ax, title, item1, item2, marker in zip(\n    axs,\n    ['train', 'val', 'test'],\n    [X_embd, Xv_embd, Xt_embd],\n    [cat_label[:4000], cat_label_v[:600],\n     cat_label_t[:900]],\n    ['.', 'x', 'x']):\n    sns.scatterplot(item1[:,0], item1[:,1], item2,\n                    hue_order = cat_list,\n                    marker=marker, palette='tab20_r', ax=ax)\n    ax.set_title(title,fontsize=20)\n    ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\nplt.savefig(ver_label+'_tsne.png', bbox_inches='tight')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This scatterplot looks like a world map of Dragon Quest and I like it.  \nThe autoencoder framework belongs to unsupervised learning and categories hasn't used for training, but the arrangements of categories in the map of train, val, and test are in good agreement.","metadata":{}},{"cell_type":"markdown","source":"## Animation\nFinally, here is the code to create the animation at the beginning of this notebook.","metadata":{}},{"cell_type":"code","source":"id_list = [3411, 1929, 8910, 8727, 3556,\n           7956, 3793, 11334, 3411]\nX_feat_inter = X_feat[id_list[0]]\nX_feat_inter = np.expand_dims(X_feat_inter, 0)\n\ninter_num=40\nfor i in range(1,len(id_list)):\n    list_temp=[]\n    for j in range(inter_num):\n        a = (j+1)/float(inter_num)\n        list_temp.append(X_feat[id_list[i-1]]*(1-a)+\n                         X_feat[id_list[i]]*a)\n    list_temp += [X_feat[id_list[i]]]*20\n    X_feat_inter = np.concatenate([X_feat_inter,\n                                   np.stack(list_temp, axis=0)])\nX_feat_inter = np.expand_dims(X_feat_inter, axis=1)    \nprint(X_feat_inter.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = X_feat_inter.shape[0]\nX_pred_inter = PCT_Decoder.predict([X_feat_inter,\n                                    eye_seed(X_feat_inter)],\n                                   batch_size=batch_size,\n                                   verbose=1)\nprint(X_pred_inter.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_inter_direct = np.expand_dims(X[id_list[0]],0)\n\nfor i in range(1,len(id_list)):\n    list_temp=[]\n    for j in range(inter_num):\n        a = (j+1)/float(inter_num)\n        list_temp.append(X[id_list[i-1]]*(1-a)+\n                         X[id_list[i]]*a)\n    list_temp += [X[id_list[i]]]*20\n    X_inter_direct = np.concatenate([X_inter_direct,\n                                   np.stack(list_temp, axis=0)])\nprint(X_inter_direct.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_gif(X_frames, gif_name):\n    frames = X_frames[0].shape[0]\n    assert frames <= 1000, f'frame_size={frames} is too large.'\n    fig = plt.figure(figsize=(12,5))\n    fig.tight_layout()\n    axs = [fig.add_subplot(121+i, projection='3d')\n           for i in range(2)]\n\n    def update(f):\n        for i, ax in enumerate(axs):\n            ax.cla()\n            ax.view_init(elev=20, azim=30+float(f)/frames*360)\n            PCPlot3d(ax, X_frames[i][f])\n            \n    plt.subplots_adjust(left=0.05, right=0.95, bottom=0, top=1)            \n    anim = FuncAnimation(fig, update, frames=frames,\n                         interval=100)\n    anim.save(gif_name, writer=\"pillow\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%matplotlib inline\nmake_gif([X_inter_direct, X_pred_inter], ver_label+\".gif\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}